## Chapter 2. 프로그램이 실행되었지만 뭐가 뭔지 하나도 모르겠다

~~그거 니가 C언어 안해서 그래~~

### Section 2.1 운영체제, 프로세스, 스레드의 근본 이해하기

모든것은 CPU에서 시작한다
> CPU는 단지 두가지 사항만 알고있다
> 1. 메모리에서 명령어를 하나 가져온다
> 2. 명령어를 실행하고 1로 돌아간다

프로그램 카운터라는 레지스터라는 메모리에서 명령어의 주소를 가져온다.

그럼 그 주소에는 물리적 메모리의 주소가 있을거다

물리적 메모리에는 컴파일러가 만든 프로그램 코드가 있을거다

그러니까 운영체제도 프로그램이라는 소리

**프로그램 로딩(간략 버전)**

프로그램을 적재할수 있는 메모리 영역을 찾는다 -> CPU 레지스터를 초기화하고 함수의 진입 포인트를 찾아 PC 레지스터를 설정한다.
~~참쉽죠?~~

이방법의 단점
- 한번에 하나의 프로그램만 실행할수 있다(DOS 생각해보자. config.sys에 SYSMEM 설정이 왜 있었을까? 프로그램 끝나면 운영체제로 돌아가기 위해서 아니었을까?)
- 모든 프로그램은 사용할 하드웨어와 드라이버를 직접 연결해야 한다.
- 입출력도 다 처리할수 있게 만들어야 한다.
- UI도 다 직접 그려야 한다.
~~여기까지 쓰니까 PDP-8이 생각난다~~

한번에 여러 프로그램 돌릴라면 각각의 프로그램을 잠깐 정지했다가 돌렸다를 반복하면 된다.

잠깐 정지하는 순간까지의 프로그램 상태를 Context라고 한다.

그리고 Context에 기록되는 정보를 단위로 묶으면 Process가 된다.

```
DOS보다 이전에, 리눅스보다 이전에, 유닉스가 멀티태스킹이 되었던 것은 애초부터 다중사용자를 상정하고 만들었기 때문
```

따라서 운영체제는 프로세스를 관리하면 되는것.

여러 프로세스가 묶여서 하나의 작업단위를 처리할때(다중 프로세스 프로그래밍), 이 방법의 단점
- 프로세스 생성할때마다 시스템 자원을 잡아먹는다(CPU와 운영체제가 관리할 거리가 늘어난다)
- 프로세스 마다 자체적은 주소공간을 소유하기에 프로세스간 통신은 코딩하기 복잡하다

~~그래서 파이프가 위대한것~~

프로세스에서 스레드로
스레드 : 하나의 프로그램에 진입점을 복수개로 두는것

```
이건 설명이 좀 이상한데?
차라리 "하나의 프로세스가 자신에게 할당받은 메모리 공간 내 존재하는 명령어를 직접 실행시키도록 운영체제가 제공하는 명령어로 감싸서 실행시킨다"이라는 표현이 맞지 않나? 
```

따라서 스레드를 돌리면 스레드 컨텍스트를 위해 메모리 공간도 소모된다. -> 이 과정에서 컴퓨팅 자원을 소모하게 되니 스레드 풀을 쓰는 것이다.

스레드 풀 사이즈를 정확하게 만들려면 각 스레드들이 CPU 자원을 얼마나 소모하는지 계산해야 한다
~~이거 웬지 게임엔진 최적화 이야기하고 비슷한거 같다~~

스레드가
- 계산만 할때 : CPU 자원만 쓰기 때문에 판단하기 쉬움
- I/O를 처리할때: Wait time + Computing time을 생각해야 한다.(그래서 ISA보다 PCI가 빠르고 PCI보다 PCIe가 빠르고 빠를수록 좋은게 그래서 그런것)

### Section 2.2 스레드간 공유되는 프로세스 리소스
스레드에 속한 스택 영역 + 프로그램 카운터 + 스택 포인터 + 스레드를 위한 레지스터 정보 = 스레드 컨텍스트

스레드는 프로세스 주소공간에서 스택 영역을 제외한 나머지 영역을 모두 공유한다 -> 왜 스택영역을 제외할까? 그랬다간 프로그램 실행이 꼬이겠지?

스레드는 프로세스 내의 데이터 영역을 공유한다. -> 그래서 메모리 할당 잘못하면 난리나는것.

스레드는 다른 스레드의 스택 프레임의 포인터를 가져와 그 값을 변경할수 있다 -> ~~응 돼도 하지마~~

또한 동적 라이브러리에게 할당된 영역도 공유한다.

```
이 섹션에 있는 코드는 전부 C++로 작성된 코드인데, 그 언어로 작성된 이유가 있는거 같다...

```
~~왜긴왜야 메모리 곡예 보여줄라고 그러지~~

### Section 2.3 스레드 안전코드는 도대체 어떻게 작성해야 할까?

하나만 기억하자: 공유 리소스를 사용할 경우에는 다른 스레드에 영향을 주지 않도록 하자. -> 공유 리소스의 범위 : 프로세스 내 데이터 부터 I/O까지 모든 컴퓨팅 자원.

영향을 주지 않게 하는 방법 -> 스레드끼리 방해하지 않게 잠금하거나 세마포어를 쓴다.
- 스레드 전용 저장소를 쓴다: 스레드끼리 격리시킨다 ~~근데 그럴거면 스레드 왜쓰냐~~
- 읽기 전용
- 원자성 연산: std::atomic
- 동기화 시 상호배제 : mutex + lock

```
스레드 세이프한 코드가 중요하긴 하지만, 스레드 전용 저장소라는 것은 결국 문제의 본질을 피해가는 것이다.
스레드는 무거운 연산이나 I/O로 인한 병목을 해결할수 있는 방법이고, 특히 I/O 병목을 피해가는데 있어서 매우 유효한 방법이라고 생각된다.
따라서 무분별한 전역 또는 메모리 참조를 통한 자원 접근이나 I/O 핸들링 보다는 락을 통해서 이 문제를 피해가는 방법 자체를 익히는 게 낫다.
```

### Section 2.4 프로그래머는 코루틴을 어떻게 이해해야 할까?
- 일반함수: 함수 중간에 return이 있음면 그 함수는 즉시 중단된다.
- 코루틴: 함수 중간에 return이 있으면 그상태에서 반환하고 일시 중단. 다시 호출하면 중단점부터 다시 시작.

추가 궁금점: 코루틴 실행시, 한번 호출이 끝나고 다음번 호출할때 새로 인자를 주면 어떻게 될까? -> 답: 새로 주어진 인자가 재시작점에 주입된다.

```
파이썬 기준으로 코루틴 일시 정지시에는 yield를 쓴다.
```

[파이썬 yield 이해하기](#파이썬-yield-이해하기)

### Section 2.5 콜백함수를 철저하게 이해한다.

~~자바 스크립트 맨처음 공부할때 이거 이해가 안되서 고생했는데, 지금 생각하면 Message Driven의 강결합버전 같은 느낌이랄지~~

콜백함수는 다른 모듈이나 스레드에서 호출한다 -> 이걸 이용해서 유연한 구조를 만들수 있구나..내 이건 미처 생각 못했소.

콜백 함수 호출시 자체적으로 콜백함수 내에서 스레드를 만들고 바로 빠져나올수 있다. -> 그런데 이렇게 하면 호출한 쪽에서 호출 결과를 받아 처리해야 하는 프로세스가 이어지지 못하니 거기에 또 콜백을 붙이고 ~~그렇게 콜백 지옥이~~

```
이거 생각해보면 대학교때 WSAAsyncSelect이용해서 소켓 프로그래밍했던거하고 비슷한 느낌이다.
```
[WSAAsyncSelect 부연설명](#WSAAsyncSelect-부연설명)

### Section 2.6 동기와 비동기를 철저하게 이해한다

- 동기: 하나 끝날때까지 기다려야 함
- 비동기: 하나 시작해놓고 다른거 하러 가도 됨

~~일할때 채팅창 켜놓고 일하는거하고 비슷한거.~~

> 컴퓨터 과학에서는 옥황상제가 문을 열어주는 동시에 적절히 창문을 닫는다 ~~문도 닫던데?~~

[FastAPI로 이해하는 동기와 비동기](#FastAPI로-이해하는-동기와-비동기)




### Section 2.7 아 맞다! 블로킹과 논 블로킹도 있다

블로킹: I/O 처리같이 처리에 시간이 오래 걸리는 작업을 수행하면 스레드 또는 프로세스가 일시 중지된다. -> python 에서 input() 이용하면 블로킹 걸린다고 생각하면 ok

논블로킹: I/O 처리하는 함수를 실행시킨 즉시 반환된다. 그러면 I/O 처리는 어떻게 해야 하는가
- 결과를 확인하는 함수를 함께 제공하고, 해당 함수를 호출하여 수신된 데이터가 있는지 확인해야 한다 -> 전역으로 I/O 버퍼를 준다는 소리인데, 결국 I/O 버퍼를 폴링해야 하는 문제를 생각해야 한다
- 데이터가 수신되면 스레드에 메시지나 신호등을 전송하는 알림 작동 방식을 사용한다 -> Win32에서 메시지 핸들러 등록하는 것을 생각해보자.
- 데이터 처리하는 함수를 콜백으로 전달한다 -> 사실상 가장많이 쓰이는 방법..일듯?

### Section 2.8 높은 동시성과 고성능을 갖춘 서버 구현

리퀘스트가 올때마다
- 다중 프로세스를 만드는 방식 : 프로그래밍이 간단하며 다중 코어 리소스 활용이 쉽지만 매번 프로세스 생성하는 부담이 크다. ~~그래서 APM 조합이 접속 폭주하면 바로 CPU부터 피크 찍는것~~
- 다중 스레드를 만드는 방식: 스레드를 쓰기때문에 좀더 가볍지만, 하나의 스레드가 강제 종료되면 다른 스레드와 프로세스도 강제 종료된다.

[스레드 공유 메모리의 위험성](#스레드-공유-메모리의-위험성)



- 이벤트 기반 프로그래밍 : 이벤트와 이벤트 핸들러를 등록해서 특정 이벤트 발생시마다 이벤트 핸들러를 호출 ~~운영체제가 다 해주시는 Win32 API 만세~~

[반응자패턴과 윈도우 운영체제의 유사성](#반응자패턴과-윈도우-운영체제의-유사성)



코루틴: 아무리 봐도 중간에 I/O 처리같은 병목이 발생할거 같은 작업들을 코루틴으로 묶어 놓으면 스케줄링 차례에 맞춰 작동하도록 만들수 있다.

~~이해하기 어려우니 차라리 코드를 만들자~~

[파이썬 코루틴 예제코드](#파이썬-코루틴-예제코드)

### Section 2.9 컴퓨터 시스템 여행: 데이터, 코드, 콜백, 클로저에서 컨테이너, 가상머신까지

> 특정언어에서 코드를 할당, 사용, 매개변수로 전달, 반환값으로 사용등 일반 변수를 다루틋이 처리할수 있을때 이 언어 함수를 일급 객체함수라 한다.

(콜백함수 + 일부 데이터) -> 변수로 취급하면 클로저 

```
뭔가 익숙하다 싶은데 이거 C++에서 제공하는 Standard Template Library와 유사한것 같다? 
```

[STL과 클로저의 연관성](#stl-and-closure)

우리가 지금은 익숙하게 쓰고 있는 컨테이너(Docker)의 개념도 프로세스와 다르지 않다.


---
참조 단락
---

# 파이썬 yield 이해하기

```python
# 이 코드도 이해해두자
def my_coroutine():
    print("코루틴 시작. 초기 인자:", (yield))
    while True:
        value = (yield)
        print("재호출. 전달된 값:", value)

# 코루틴 생성
co = my_coroutine()

# 첫 호출: 초기 인자 전달
next(co) # 또는 co.send(None)
# 출력: 코루틴 시작. 초기 인자: None

# 재호출: 새로운 값 전달
co.send("Hello")
# 출력: 재호출. 전달된 값: Hello

# 또 다른 재호출
co.send(123)
# 출력: 재호출. 전달된 값: 123
```
---

# WSAAsyncSelect 부연설명

WSAAsyncSelect를 이용한 네트워크 프로그래밍과 콜백 함수를 이용한 프로그래밍은 모두 비동기 I/O를 구현하는 방식이라는 점에서 유사점을 가집니다. 그러나 이벤트 처리 방식에서 주요한 차이점을 보입니다.

유사점
- 논블로킹(Non-blocking) 동작: 두 방식 모두 I/O 작업(예: 데이터 수신, 연결 수락)이 완료될 때까지 프로그램의 실행 흐름이 멈추지 않습니다.
- 이벤트 기반: 특정 I/O 이벤트(예: 데이터 도착, 연결 끊김)가 발생했을 때 미리 정의된 동작을 수행합니다.
- 리소스 효율성: I/O 작업이 완료될 때까지 기다리느라 CPU 시간을 낭비하지 않으므로, 다수의 동시 연결을 효율적으로 처리할 수 있습니다.

차이점
이벤트 처리 메커니즘:
- WSAAsyncSelect: I/O 이벤트가 발생하면 윈도우 메시지 형태로 프로그램에 통보됩니다. 개발자는 윈도우 프로시저에서 특정 메시지(예: WM_SOCKET)를 처리하는 코드를 작성해야 합니다. 이 방식은 윈도우 메시지 루프를 기반으로 합니다.
- 콜백 함수: I/O 이벤트가 발생하면 운영체제나 라이브러리가 미리 등록된 콜백 함수를 직접 호출합니다. 개발자는 이벤트 종류별로 실행될 함수를 정의하고, 이를 비동기 I/O 함수에 인자로 넘겨주어야 합니다.

프로그래밍 구조:
- WSAAsyncSelect: 메시지 루프와 윈도우 프로시저를 중심으로 설계됩니다. 모든 이벤트 처리가 하나의 메시지 핸들러 함수 내에서 이루어지므로, 코드가 복잡해지면 가독성이 떨어질 수 있습니다.
- 콜백 함수: 각 I/O 작업(예: WSARecv)마다 콜백 함수를 등록하는 방식으로 설계됩니다. 이 구조는 이벤트별로 코드를 분리할 수 있어 코드의 모듈성과 재사용성이 더 높습니다.

사용 환경:

WSAAsyncSelect: 주로 Win32 API를 사용하는 윈도우 환경에서 사용됩니다.

콜백 함수: 다양한 운영체제와 라이브러리(예: Linux의 libuv, Windows의 I/O Completion Ports와 함께 사용되는 콜백)에서 널리 사용되는 범용적인 패턴입니다.

요약하자면, WSAAsyncSelect는 윈도우 메시지 루프를 이용한 콜백 패턴의 한 형태라고 볼 수 있습니다. 그러나 일반적인 콜백 함수 방식은 함수 포인터를 직접 전달하는 보다 직접적인 구조를 사용하며, 윈도우 시스템에 종속적이지 않다는 점에서 차이가 있습니다.

---


# FastAPI로 이해하는 동기와 비동기

Q: FastAPI에서 Async/Await를 이용해서 프로그래밍 하면, 해당 리퀘스트를 요청한 클라이언트와 하나의 작업단위를 만들어 놓고 FastAPI는 다른 작업을 처리한다는 것인가?

A: 네, 맞습니다. FastAPI에서 Async/Await를 사용하면, 해당 리퀘스트를 처리하는 동안 I/O 작업(예: 데이터베이스 조회, 외부 API 호출 등)이 발생했을 때, 해당 작업이 완료될 때까지 **기다리지 않고** 다른 요청을 처리할 수 있습니다. 이것이 비동기 프로그래밍의 핵심 원리입니다.

### 비동기 프로그래밍의 작동 원리

* **Non-blocking I/O**: 전통적인 동기(Synchronous) 방식은 하나의 요청이 들어오면 I/O 작업이 완료될 때까지 해당 스레드가 블록(Block)됩니다. 즉, 다른 요청을 처리할 수 없습니다. 
* **Async/Await**: `async def`로 정의된 함수는 비동기 함수이며, 이 함수 내에서 `await` 키워드를 사용하면, 해당 I/O 작업이 완료되기를 기다리는 동안 스레드가 블록되지 않고 **다른 I/O 작업이나 요청을 처리**할 수 있도록 제어권을 이벤트 루프에 넘깁니다. 
* **이벤트 루프**: FastAPI의 기반이 되는 ASGI(Asynchronous Server Gateway Interface) 서버(예: Uvicorn)는 이벤트 루프를 사용하여 비동기 작업을 스케줄링합니다. 이벤트 루프는 `await` 지점에서 제어권을 넘겨받아 대기 중인 다른 작업을 처리하고, `await`된 작업이 완료되면 다시 해당 함수로 돌아와 이어서 실행합니다.

따라서, "해당 리퀘스트를 요청한 클라이언트와 하나의 작업단위를 만들어 놓고 FastAPI는 다른 작업을 처리한다"는 것은 정확한 표현입니다. 이는 하나의 프로세스 또는 스레드에서 **동시에 많은 I/O 바운드 작업**을 효율적으로 처리할 수 있게 해줍니다. 이로 인해 동시성(Concurrency)이 향상되고, 서버의 처리량(Throughput)이 크게 증가합니다.


Q: 그렇다면 서버에 request가 급증하는 시나리오 상에서 FastAPI가 동기/비동기 요청을 처리할때 어떻게 작동하는지 설명해주게. 단, FastAPI 자체는 ubicorn만 이용해서 가동한다고 상정한다.
A: FastAPI는 Uvicorn을 이용해 가동될 때, 동기 및 비동기 요청을 처리하는 방식에 차이가 있습니다. FastAPI는 기본적으로 **이벤트 루프 기반의 비동기 I/O**를 사용하지만, 동기 함수를 처리하기 위해 **별도의 스레드풀**을 활용합니다.

### 비동기 요청 처리 (Async)

* **Async/Await**: 비동기 엔드포인트(`async def`)는 **이벤트 루프**에 의해 관리됩니다. Uvicorn은 **하나의 메인 스레드**에서 이벤트 루프를 실행하며, 이 루프는 여러 동시 요청을 처리합니다.
* **Non-blocking I/O**: 요청 처리 중 I/O 작업(예: DB 접근, 외부 API 호출)을 만나 `await`를 호출하면, **제어권이 이벤트 루프에 반환**됩니다. 이벤트 루프는 해당 I/O 작업이 완료될 때까지 기다리지 않고, 다른 대기 중인 요청을 처리합니다.
* **높은 동시성**: I/O 바운드 작업이 많은 시나리오에서, 이벤트 루프는 매우 효율적으로 **동시에 수많은 요청**을 처리할 수 있습니다. 스레드나 프로세스를 많이 생성하지 않아도 되므로, 리소스 소모가 적고 처리량이 높습니다.


### 동기 요청 처리 (Sync)

* **스레드풀 사용**: 동기 엔드포인트(`def`)는 이벤트 루프를 **블록**할 수 있습니다. 이를 방지하기 위해 FastAPI는 `def` 함수를 별도의 **스레드풀(ThreadPoolExecutor)**에서 실행합니다.
* **블로킹 방지**: 요청이 동기 함수로 들어오면, Uvicorn은 이 함수를 **스레드풀의 사용 가능한 스레드**에 할당합니다. 함수는 해당 스레드에서 완전히 실행되며, 이 시간 동안 메인 이벤트 루프는 블록되지 않고 계속 다른 요청을 처리합니다.
* **스케일링 제한**: 스레드풀의 스레드 수는 **제한**되어 있습니다. 따라서 요청이 급증하여 스레드풀의 모든 스레드가 사용 중이라면, 새로운 동기 요청은 **대기열에 쌓이게** 됩니다. 이로 인해 비동기 요청에 비해 동시성 처리 능력이 낮아질 수 있습니다.

### 요약

결론적으로, 요청이 급증하는 상황에서 I/O 작업이 많은 경우 **비동기 엔드포인트**는 훨씬 더 높은 처리량과 안정성을 보여줍니다. 반면, CPU 연산이 많은 작업은 동기 엔드포인트로 처리하는 것이 효율적입니다. FastAPI는 이 두 방식을 모두 지원하여 개발자가 상황에 맞게 최적의 방식을 선택할 수 있게 합니다.

| 특징 | 비동기 요청 (async def) | 동기 요청 (def) |
| :--- | :--- | :--- |
| **처리 방식** | 이벤트 루프 (Non-blocking) | 스레드풀 (Blocking) |
| **리소스** | 단일 스레드 (이벤트 루프) | 다중 스레드 (스레드풀) |
| **성능** | I/O 바운드 작업에 매우 효율적 | CPU 바운드 작업에 적합 |
| **급증 시** | 많은 요청을 동시에 처리 가능 | 스레드풀 포화 시 요청 지연 발생 |


Q: 그런데 FastAPI 공식문서에는 비동기 방식은 주의해서 사용하라 되어있는데 그 이유는 무엇이라 생각하는가?
A: FastAPI 공식 문서는 비동기 방식의 오용에 대한 주의를 주는 것으로 보입니다. 그 이유는 비동기 프로그래밍이 모든 상황에서 만능 해결책이 아니며, 잘못 사용하면 오히려 성능 저하나 복잡성 증가를 초래할 수 있기 때문입니다.

### 1. 비동기/동기 함수의 혼합 사용

FastAPI는 비동기 함수와 동기 함수를 모두 지원하지만, 이 둘을 **부적절하게 혼합**하면 문제가 발생합니다. 예를 들어, `async def`로 정의된 함수 내에서 `await` 없이 `requests`와 같은 동기 라이브러리를 사용하면, 해당 블로킹 작업이 끝날 때까지 이벤트 루프가 멈춰버립니다. 이는 비동기 프로그래밍의 가장 큰 장점인 동시성을 잃게 만듭니다.

### 2. 모든 작업이 I/O 바운드 작업이 아니다

비동기 프로그래밍은 주로 **I/O 바운드(I/O-bound)** 작업에 최적화되어 있습니다. 데이터베이스 접근, 네트워크 요청, 파일 I/O 등과 같이 작업이 완료될 때까지 기다리는 시간이 긴 작업들이 여기에 해당됩니다. 하지만 CPU 연산이 많은 **CPU 바운드(CPU-bound)** 작업(예: 복잡한 수학 연산, 이미지 처리, 암호화)은 비동기 방식으로 처리해도 성능상 이점을 얻기 어렵습니다. 오히려 컨텍스트 스위칭 비용만 추가되어 성능이 저하될 수 있습니다.

### 3. 복잡성 증가와 디버깅의 어려움

비동기 코드는 동기 코드에 비해 구조가 복잡해지기 쉽습니다. 특히 콜백 지옥(Callback hell)과 유사한 **await hell**이나, 비동기 컨텍스트에서 예외 처리가 예상치 않게 동작하는 등 **디버깅이 어려울 수 있습니다.** 또한, 비동기 생태계에 대한 이해가 부족하면 예상치 못한 버그를 발생시킬 가능성이 큽니다.

### 결론

요약하자면, FastAPI가 비동기 방식에 대한 주의를 주는 것은 개발자가 **비동기 프로그래밍의 본질**을 이해하고, 자신의 애플리케이션에 적합한 상황(특히 I/O 바운드 작업)에서 올바르게 사용하도록 유도하기 위함입니다. 무분별한 `async/await` 사용은 오히려 동기 방식보다 나쁜 결과를 초래할 수 있습니다.

---

# 스레드 공유 메모리의 위험성

하나의 스레드 오류가 전체 프로세스를 종료시키는 시나리오를 자바 서블릿 컨테이너를 예시로 들어 설명해 드릴게요. 이 시나리오는 실제 서비스에서는 흔히 발생하지 않지만, **스레드 공유 메모리의 위험성**을 잘 보여줍니다.

### 시나리오: 공유 자원 접근 오류

사용자 세션 정보를 저장하고 조회하는 서블릿이 있다고 가정해 봅시다. 이 서블릿은 모든 스레드가 공유하는 `static` 변수를 사용하여 세션 데이터를 관리합니다.

#### **서블릿의 내부 동작**

```java
import java.util.concurrent.ConcurrentHashMap;

public class SharedResourceServlet extends HttpServlet {
    // 🚨 문제의 원인: 모든 스레드가 공유하는 static 변수
    private static ConcurrentHashMap<String, String> sessionData = new ConcurrentHashMap<>();

    @Override
    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {
        String sessionId = request.getSession().getId();
        
        // 1. 세션 데이터 저장 (공유 자원에 쓰기)
        sessionData.put(sessionId, "user_profile_data");
        
        // 2. 다른 스레드에서 치명적인 오류 발생 (가상 시나리오)
        // **이 부분이 핵심입니다.**
        // 만약 다른 스레드가 'sessionData' 객체를 조작하는 과정에서 
        // 널(null) 포인터 예외와 같은 치명적인 오류를 발생시켰다고 가정해 봅시다.
        try {
            if (sessionId.equals("critical_session")) {
                // 이 가상의 코드는 JVM을 충돌시킬 수 있는 메모리 오류를 유도합니다.
                // 현실에서는 이런 코드를 직접 작성하지 않지만, 외부 라이브러리나
                // JNI(Java Native Interface) 호출 과정에서 발생할 수 있습니다.
                // 예를 들어, JNI를 통해 접근한 C++ 코드에서 메모리 손상이 발생했다고 가정.
                throw new NullPointerException("Fatal error in a shared resource access");
            }
        } catch (Exception e) {
            // 이 예외는 catch 블록으로 잡히지만, JVM이 복구할 수 없는
            // 'fatal error'를 발생시켰다고 가정합니다.
            // (예: java.lang.OutOfMemoryError, java.lang.StackOverflowError)
            e.printStackTrace();
        }
        
        // 3. 정상적인 응답 (그러나 이미 JVM은 불안정한 상태)
        response.getWriter().write("Session data saved successfully.");
    }
}
```
#### **치명적인 오류 발생 시나리오**

1.  **공유 자원 접근**: 여러 사용자가 동시에 `SharedResourceServlet`에 요청을 보냅니다. 서블릿 컨테이너는 요청마다 새로운 스레드를 할당하고, 이 스레드들은 **하나의 `sessionData` 객체**에 접근합니다.

2.  **스레드 A의 치명적 오류**: 스레드 A가 "critical\_session" 요청을 처리하는 중에 `static` `sessionData`에 접근하다가 **JVM이 복구할 수 없는 치명적인 오류**(`StackOverflowError` 또는 외부 라이브러리로 인한 메모리 손상)를 일으킵니다.

3.  **프로세스 전체 종료**: JVM은 프로세스 내의 모든 스레드가 공유하는 힙(Heap) 메모리 영역에 접근합니다. 스레드 A가 발생시킨 치명적인 오류는 JVM의 안정성을 해치고, **결국 JVM 프로세스 전체가 강제 종료됩니다.**

4.  **다른 스레드의 종료**: 프로세스가 종료되므로, 스레드 A 외에 다른 모든 스레드(B, C, D 등)도 현재 처리 중인 요청을 완료하지 못하고 **함께 강제 종료**됩니다. 이로 인해 모든 클라이언트는 예기치 않은 연결 끊김 또는 오류 응답을 받게 됩니다.

이 시나리오는 스레드 간 **메모리 공유**의 양면성을 보여줍니다. 데이터 공유가 용이하다는 장점이 있지만, 하나의 스레드에서 발생한 메모리 관련 오류가 전체 시스템을 무너뜨릴 수 있다는 치명적인 위험을 안고 있습니다. 이것이 바로 Nginx와 같은 서버들이 스레드 대신 프로세스를 사용하여 독립성을 확보하는 이유입니다.

from 제미니

---


# 반응자패턴과-윈도우-운영체제의-유사성

Q: 다중 스레드에서 실행되는 이벤트 핸들러 모델을 반응자 패턴이라고 하는데, 이와 Win32 API가 작동하는 방식이 유사하다고 할수 있겠는가?
A: 네, 다중 스레드에서 실행되는 이벤트 핸들러 모델을 **반응자(Reactor) 패턴**이라고 하며, Win32 API의 작동 방식은 이와 유사하다고 할 수 있습니다. 

---

### 반응자 패턴 (Reactor Pattern)

반응자 패턴은 **이벤트 기반 비동기 I/O**를 구현하는 디자인 패턴입니다. 이 패턴의 핵심은 단일 스레드(또는 몇 개의 스레드)가 여러 I/O 이벤트 소스를 모니터링하고, 특정 이벤트가 발생하면 해당 이벤트를 처리할 **핸들러(Handler)**를 호출하는 것입니다.

* **반응자(Reactor)**: 이벤트 소스들을 모니터링하는 주체입니다.
* **이벤트 핸들러(Event Handler)**: 이벤트가 발생했을 때 실행되는 콜백 함수입니다.

### Win32 API와의 유사성

Win32 API는 `GetMessage` 또는 `PeekMessage` 함수를 중심으로 **메시지 루프**를 실행합니다. 이는 반응자 패턴의 동작과 매우 유사합니다.

* **반응자(Reactor) ↔ 메시지 루프**: `GetMessage` 함수는 운영체제가 보낸 메시지(이벤트)가 도착할 때까지 대기합니다. 이는 반응자 패턴에서 이벤트 소스를 모니터링하는 역할과 같습니다.
* **이벤트 소스 ↔ 메시지 큐**: 키보드 입력, 마우스 클릭, 윈도우 크기 변경, 네트워크 소켓 이벤트(예: `WSAAsyncSelect`로 등록된 이벤트) 등 다양한 이벤트들이 **메시지 큐**에 쌓입니다.
* **이벤트 핸들러 ↔ 윈도우 프로시저**: 메시지 루프가 메시지를 읽으면, 해당 메시지를 처리하기 위해 **윈도우 프로시저(Window Procedure)**를 호출합니다. 이 윈도우 프로시저가 바로 이벤트 핸들러 역할을 수행합니다.

따라서, **Win32 API의 메시지 기반 프로그래밍 모델은 반응자 패턴의 대표적인 예시**라고 할 수 있습니다. 두 모델 모두 I/O 작업이나 외부 이벤트가 발생했을 때 프로그램의 제어권을 넘겨받아 미리 정의된 핸들러(또는 프로시저)를 실행하는 방식으로 동작합니다.

from 제미니

---

# 파이썬 코루틴 예제코드

```python
import asyncio
import time

# I/O 바운드 작업을 시뮬레이션하는 비동기 함수
# 실제로는 네트워크 호출이나 데이터베이스 쿼리가 이 자리를 대신합니다.
async def get_user_info():
    """사용자 정보를 가져오는 비동기 함수."""
    print(f"[{time.strftime('%H:%M:%S')}] GetUserInfo: 시작")
    await asyncio.sleep(2)  # 2초 동안 I/O 작업 대기
    print(f"[{time.strftime('%H:%M:%S')}] GetUserInfo: 완료")
    return {"user_name": "Alice"}

async def get_query_info():
    """쿼리 정보를 가져오는 비동기 함수."""
    print(f"[{time.strftime('%H:%M:%S')}] GetQueryInfo: 시작")
    await asyncio.sleep(1)  # 1초 동안 I/O 작업 대기
    print(f"[{time.strftime('%H:%M:%S')}] GetQueryInfo: 완료")
    return {"query_count": 5}

async def get_stock_info():
    """주식 정보를 가져오는 비동기 함수."""
    print(f"[{time.strftime('%H:%M:%S')}] GetStockInfo: 시작")
    await asyncio.sleep(3)  # 3초 동안 I/O 작업 대기
    print(f"[{time.strftime('%H:%M:%S')}] GetStockInfo: 완료")
    return {"stock_price": 15000}

async def main():
    """세 개의 비동기 작업을 동시에 실행하고 결과를 취합하는 메인 함수."""
    print(f"[{time.strftime('%H:%M:%S')}] 모든 코루틴 실행 시작")

    # asyncio.gather()를 사용하여 세 코루틴을 묶어 병렬 실행
    results = await asyncio.gather(
        get_user_info(),
        get_query_info(),
        get_stock_info()
    )

    # results는 각 코루틴의 반환 값을 순서대로 담고 있는 리스트
    user_info, query_info, stock_info = results

    print(f"[{time.strftime('%H:%M:%S')}] 모든 코루틴 완료")
    print(f"사용자 정보: {user_info}")
    print(f"쿼리 정보: {query_info}")
    print(f"주식 정보: {stock_info}")

# 메인 이벤트 루프를 실행하여 프로그램 시작
if __name__ == "__main__":
    # Python 3.7 이상
    asyncio.run(main())

    # Python 3.6 이하
    # loop = asyncio.get_event_loop()
    # loop.run_until_complete(main())
```
```
[16:22:27] 모든 코루틴 실행 시작
[16:22:27] GetUserInfo: 시작
[16:22:27] GetQueryInfo: 시작
[16:22:27] GetStockInfo: 시작 -> 여기까지 거의 같은 시간에 실행되었다.
[16:22:28] GetQueryInfo: 완료
[16:22:29] GetUserInfo: 완료
[16:22:30] GetStockInfo: 완료
[16:22:30] 모든 코루틴 완료 -> 모든 코루틴이 완료된 후 결과가 출력된다.
사용자 정보: {'user_name': 'Alice'}
쿼리 정보: {'query_count': 5}
주식 정보: {'stock_price': 15000}


** Process exited - Return Code: 0 **

```

# stl and closure

클로저(closure)와 C++의 람다 표현식(lambda expressions)은 매우 유사한 개념이며, 이 람다 표현식은 C++의 STL(Standard Template Library)과 함께 사용될 때 강력한 기능을 제공합니다.

### 클로저와 람다의 유사성

클로저는 **외부 함수의 환경을 기억하는 내부 함수**를 의미합니다. 함수가 정의될 당시의 스코프에 있는 변수들을 자신의 컨텍스트에 캡처(capture)하여, 외부 함수가 종료된 후에도 그 변수들을 사용할 수 있습니다.

C++의 람다 표현식도 이와 매우 유사합니다. 람다는 익명 함수를 정의하는 문법으로, `[]` 대괄호 안에 \*\*캡처 리스트(capture list)\*\*를 통해 외부 변수들을 캡처할 수 있습니다. 람다 또한 자신이 정의된 스코프의 변수들을 "기억"하고 사용할 수 있다는 점에서 클로저와 본질적으로 동일한 역할을 합니다.

예시:

```cpp
#include <iostream>
#include <vector>
#include <algorithm>

int main() {
    int x = 10;
    std::vector<int> nums = {1, 2, 3, 4, 5};

    // 람다 표현식 (클로저 역할)
    // [x]는 외부 변수 x를 값으로 캡처
    std::for_each(nums.begin(), nums.end(), [x](int n) {
        std::cout << n + x << " ";
    });

    return 0;
}
```

위 코드에서 `[x](int n) { ... }`는 람다 표현식입니다. 이 람다는 외부 변수 `x`를 캡처하여 `main` 함수가 종료된 후에도 `std::for_each` 알고리즘이 `x`의 값을 사용할 수 있게 합니다.

### STL과의 연관성

C++11 이후 도입된 람다 표현식은 **STL과 함께 사용될 때** 그 진가를 발휘합니다. 이전에는 STL 알고리즘(`std::for_each`, `std::transform`, `std::find_if` 등)에 사용자 정의 함수 객체(functor)를 넘겨줘야 했지만, 람다 덕분에 복잡한 클래스를 정의할 필요 없이 인라인으로 간단하게 함수를 정의할 수 있게 되었습니다.

결론적으로, 클로저 함수가 C++의 람다 표현식을 떠올리게 하는 것은 매우 자연스러운 연상입니다. 그리고 이 람다 표현식이 STL의 알고리즘과 결합하여 효율적인 코드를 작성하는 데 핵심적인 역할을 한다는 점에서, 두 개념은 깊은 연관성을 가집니다.